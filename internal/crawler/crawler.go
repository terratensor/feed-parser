package crawler

import (
	"fmt"
	"log"
	"math/rand"
	"strings"
	"time"

	"github.com/gocolly/colly/v2"
	"github.com/terratensor/feed-parser/internal/config"
	"github.com/terratensor/feed-parser/internal/entities/feed"
	"github.com/terratensor/feed-parser/internal/metrics"
)

// VisitMil –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
func VisitMil(entry *feed.Entry, config *config.Crawler, metrics *metrics.Metrics) (*feed.Entry, error) {
	c := colly.NewCollector()

	c.AllowURLRevisit = true

	// –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º User-Agent –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
	c.UserAgent = config.UserAgent

	// –°—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫
	retryCount := 0

	// –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
	c.OnError(func(r *colly.Response, err error) {
		if retryCount < config.MaxRetries {
			retryCount++
			log.Printf("Error: %v. Retrying (%d/%d) in %v...", err, retryCount, config.MaxRetries, config.RetryDelay)
			// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ —Å –Ω–æ–º–µ—Ä–æ–º –ø–æ–ø—ã—Ç–∫–∏
			metrics.ErrorRequests.WithLabelValues(r.Request.URL.String(), err.Error(), fmt.Sprintf("%d", retryCount)).Inc()
			time.Sleep(config.RetryDelay)
			r.Request.Retry()
		} else {
			log.Printf("Error: %v. Max retries reached (%d).", err, config.MaxRetries)
		}
	})

	c.OnRequest(func(r *colly.Request) {
		fmt.Println("Crawl on Page", r.URL.String())
	})

	// –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –∑–∞–¥–µ—Ä–∂–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
	c.Limit(&colly.LimitRule{
		RandomDelay: time.Duration(config.RandomDelayMin+rand.Intn(config.RandomDelayMax-config.RandomDelayMin)) * time.Second,
	})

	c.OnHTML("#center", func(e *colly.HTMLElement) {
		log.Printf("Crawling Url %#v", entry.Url)
		title := e.ChildText("h1")

		// –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
		content := e.ChildTexts("p")

		author := e.ChildText("div a.date")

		var sb strings.Builder
		for _, con := range content {
			if len(con) > 0 {
				sb.WriteString("<p>")
				sb.WriteString(con)
				sb.WriteString("</p>")
			}
		}

		entry.Title = title
		entry.Content = sb.String()
		entry.Author = author

		log.Printf("Crawling Title: %v", entry.Title)
	})

	// –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É —Å–Ω–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
	n := config.SleepMin + rand.Intn(config.SleepMax-config.SleepMin)
	d := time.Duration(n)
	time.Sleep(d * time.Second)

	// –ü–æ—Å–µ—â–∞–µ–º URL
	err := c.Visit(entry.Url)
	if err != nil {
		log.Printf("Crawler Error: %v", err)
		// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ —Å –Ω–æ–º–µ—Ä–æ–º –ø–æ–ø—ã—Ç–∫–∏
		metrics.ErrorRequests.WithLabelValues(entry.Url, err.Error(), fmt.Sprintf("%d", retryCount)).Inc()
		return nil, err
	}

	// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
	metrics.SuccessRequests.WithLabelValues(entry.Url, fmt.Sprintf("%d", retryCount)).Inc()

	return entry, nil
}

func VisitMid(entry *feed.Entry, config *config.Crawler, metrics *metrics.Metrics) (*feed.Entry, error) {

	c := colly.NewCollector()
	c.AllowURLRevisit = false

	c.UserAgent = ""
	//c.UserAgent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"

	c.OnRequest(func(r *colly.Request) {
		fmt.Println("Crawl on Page", r.URL.String())
	})

	// iterating over the list of industry card

	// HTML elements
	c.Limit(&colly.LimitRule{
		Delay:       5 * time.Second,
		RandomDelay: 10 * time.Second,
	})

	c.OnHTML("div.photo-content", func(e *colly.HTMLElement) {
		log.Printf("Crawling Url %#v", entry.Url)

		title := e.ChildText("h1.photo-content__title")

		number := e.ChildText("p.article-line__note.article-line__note_small")

		// filter out unwanted data
		content := e.ChildTexts("div.text.article-content p")

		var sb strings.Builder
		for _, con := range content {
			if len(con) > 0 {
				sb.WriteString("<p>")
				sb.WriteString(con)
				sb.WriteString("</p>")
			}
		}

		entry.Title = title
		entry.Content = sb.String()
		entry.Number = number

		log.Printf("Mid photo-content: %v", entry.Title)
	})

	// –ï—Å–ª–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –∫–∞–∫ –∞–Ω–æ–Ω—Å
	c.OnHTML("ul.announcements", func(e *colly.HTMLElement) {
		log.Printf("Crawling Url %#v", entry.Url)

		title := e.ChildText("h3.announcement__title")

		number := e.ChildText("div.announcement__doc-num")

		// filter out unwanted data
		content := e.ChildTexts("div.announcement__text > p")

		var sb strings.Builder
		for _, con := range content {
			if len(con) > 0 {
				sb.WriteString("<p>")
				sb.WriteString(con)
				sb.WriteString("</p>")
			}
		}

		entry.Title = title
		entry.Content = sb.String()
		entry.Number = number

		log.Printf("Mid announcements: %v", entry.Title)
	})

	count := 0
	for {
		// –æ–∂–∏–¥–∞–µ–º –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞ —Ä–∞–Ω–¥–æ–º–Ω–æ 10 - 30 —Å–µ–∫—É–Ω–¥
		// —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –ø–∞—É–∑—ã –ø–æ—Å–ª–µ –Ω–µ—É–¥–∞—á–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏
		var n int
		if c.AllowURLRevisit {
			n = (1+count)*10 + rand.Intn(30)
		} else {
			n = ((1 + count) * 10) + rand.Intn(30)
		}
		d := time.Duration(n)
		time.Sleep(d * time.Second)

		// –ü–æ—Å–µ—â–∞–µ—Ç —Å—Å—ã–ª–∫—É, –µ—Å–ª–∏ –æ—à–∏–±–∫–∞, –æ–±—ã—á–Ω–æ connection reset by peer,
		// —Ç–æ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –ø–æ–ø—ã—Ç–∫—É –∏ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫,
		// –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç 10 —Ä–∞–∑
		err := c.Visit(entry.Url)
		if err != nil {
			count++
			log.Printf("Crawler Error: %v", err)

			if c.AllowURLRevisit && count <= config.MaxRetries {
				// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ —Å –Ω–æ–º–µ—Ä–æ–º –ø–æ–ø—ã—Ç–∫–∏
				metrics.ErrorRequests.WithLabelValues(entry.Url, err.Error(), fmt.Sprintf("%d", count)).Inc()
				log.Printf("üîÑ try again: %v url: %v", count, entry.Url)
				continue
			}

			// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ —Å –Ω–æ–º–µ—Ä–æ–º –ø–æ–ø—ã—Ç–∫–∏
			metrics.ErrorRequests.WithLabelValues(entry.Url, err.Error(), fmt.Sprintf("%d", count)).Inc()

			return nil, err
		}
		break
	}

	// –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
	metrics.SuccessRequests.WithLabelValues(entry.Url, fmt.Sprintf("%d", count)).Inc()

	return entry, nil
}
